{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron\n",
    "Before going to MLP, lets go through some required basics. If you consider linear binary classification problem where we have two features $x_1$, $x_2$ and a binary target (+1 or -1), the decision function, $d(x) = w_0 +w_1 x_1 + w_2 x_2)$ would be a linear combination of the features. \n",
    "\n",
    "To predict whether a data point is positive or negative we try to find a separating line represented by $d(x)$ and use it sign, $sign(d(x))$ to tell which side of the line the point falls. However, if $d(x)$ = 0 exactly then the point lies exactly on the separating plane.\n",
    "\n",
    "<img src=\"images/linear_binary_classification.jpg\" style=\"width:10%\">\n",
    "\n",
    "With this idea in mind, we can illustrate logistic regression in a similar way. Here it calculates the probability of a point being plus or minus 1. It uses a function called sigmoid for this purpose instead of a sign function. Sigmoid function transforms any value to the range $[0,1]$, to valid probabilities.\n",
    "\n",
    "$σ(d(x))$,where $σ(x) = 1/1 + exp(-x)$\n",
    "\n",
    "The distance between the point and the line can be converted into confidence. The farther the point the more confident the model is that it belongs to that class. So, any point that lies on the line, $d(x)=0$ has $\\sigma(x)$ as 0.5 meaning it can be either plus or minus and model is not confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a rather complex problem below on the left, we try to divide it into subproblems of 3 logistic regressions represented by lines $z_1$, $z_2$ and $z_3$. The predictions of these lines would be used as new features in our model. In short, we translate every data point from having 2 co-ordinates $(x_1, x_2)$ to 3 co-ordinates $(z_1, z_2, z_3)$. To solve this, we build a linear model on top of these new features:\n",
    "$(x_1, x_2)$ –> $(z_1, z_2, z_3)$\n",
    "\n",
    "<img src=\"images/complex_problem.jpg\" style=\"width:10%\">     <img src=\"images/complex_solution.jpg\" style=\"width:10%\">\n",
    "\n",
    "$z_i = \\sigma(w_{0,i} + w_{1,i} x_1 + w_{2,i} x_2)$\n",
    "\n",
    "$a(x) = \\sigma(w_0 + w_1z_1(x) + w_2z_2(x) +w_3z_3(x)$\n",
    "\n",
    "We then apply 3 linear models to get z1, z2, z3 and then a final one to get our final prediction ‘a’. This composite function could be translated into a computational graph:\n",
    "\n",
    "<img src=\"images/computational_graph.jpg\" style=\"width:10%\"> \n",
    "Nodes are the computed variables $(x_1, x_2, z_1, z_2, z_3, a)$. Edges correspond to dependencies. Meaning, we need $x_1$ and $x_2$ to compute $z_1$ or $z_2$ or $z_3$ and these in turn to compute a. This computational graph is a **multilayer perceptron**. \n",
    "\n",
    "It is the simplest artificial neural network. It has 3 basic layers: Input layer of input features $x_1, x_2$; first hidden layer of $z_1, z_2, z_3$, also called as dense layer or fully connected layer; and output layer of prediction ‘a’.  Each node is called a neuron. A neuron is something that takes a linear combination of inputs and applies some non-linear activation function such as sigmoid activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how we arrived with the terms, artificial neurons.\n",
    "\n",
    "A neuron in our brain is some complex cell that get signals from other cells similar to it and based on some difficult logic inside of that cell it decides to output a signal or not which is then transmitted to other neurons like it.\n",
    "<img src=\"images/artificial_neuron.jpg\" style=\"width:10%\"> \n",
    "\n",
    "We try to approximate this process with a mathematical neuron i.e, an artificial neuron using logistic regression. Suppose we have $x_1$ and $x_2$ as inputs and multiply by the weights and compute the sum. We apply sigmoid function which is a smooth indicator approximation. It that decides whether to output a signal or not based on if it is positive or not. We smooth the indicator so that our problem becomes differentiable. It is called activation function since it activates the neuron when a certain sum is computed. Here the artificial neuron is correlation activated since when the input has some pattern similar to what it is trying to find then it has a huge activation in the output. We need the non-linear activation function otherwise any algorithm could just be a fancy linear function that is no better than a linear model and we cannot solve complex problems with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of layers, neurons in each layer and the activation function constitute an architecture of MLP. These are also hyperparameters that we need to choose in order to define an architecture.\n",
    "\n",
    "To train an MLP we can use stochastic gradient descent(SGD) since each neuron which is a linear combination of inputs is differentiable and the activation function is also differentiable. This just turns out to be a complex function that is still differentiable, and we can use SGD. One possible problem could be that we can may have lot of local optimums and have a sub-optimal solution in which case the best decision would be to start from some other point that could lead to global optimum.\n",
    "\n",
    "Regarding training an MLP we need to be able to 1) automatically calculate the gradients since they would be many hidden layers 2) We need to calculate them fast since we would have many neurons.\n",
    "\n",
    "To calculate the gradients automatically, we use chain rule:\n",
    "\n",
    "For a composite function:\n",
    "$z_1=z_1(x_1, x_2)$, $z_2=z_2(x_1, x_2)$, $p=p(z_1, z_2)$ where $z_1, z_2, z_3, p$ are differentiable. As per chain rule:\n",
    "\n",
    "$\\frac{∂p}{∂x1}=\\frac{∂p}{∂z1} \\frac{∂z1}{∂x1}+ \\frac{∂p}{∂z2} \\frac{∂z2}{∂x1}$\n",
    "\n",
    "If we construct a new graph of derivatives it will be the same as computational graph with one distinction. The edges will change their direction with each edge assigned to a derivative.  <img src=\"images/graph_of_derivatives.jpg\" style=\"width:10%\"> \n",
    "\n",
    "If we take a deeper network, then to get $\\frac{∂p}{∂x1}$ we apply chain rule recursively since the derivatives along the way are also composite functions. **Each term in our derivative that we have computed actually corresponds to some path going from p to x1**. <img src=\"images/deeper_comp_network.jpg\" style=\"width:10%\"> \n",
    "\n",
    "$\\frac{∂p}{∂x1}=\\frac{∂p}{∂h1} \\frac{∂h1}{∂z1} \\frac{∂z1}{∂x1}+ \\frac{∂p}{∂h1} \\frac{∂h1}{∂z2} \\frac{∂z2}{∂x1}+ \\frac{∂p}{∂h2} \\frac{∂h2}{∂z1} \\frac{∂z1}{∂x1}+ \\frac{∂p}{∂h2} \\frac{∂h2}{∂z2} \\frac{∂z2}{∂x1}$\n",
    "\n",
    "Therefore, using chain rule and computational graph, we compute derivatives of composite functions **automatically**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to be able to apply chain rule **efficiently** to compute the derivatives faster since they could be billions of parameters. In the final step, in order to calculate derivative of prediction with respect to the first node, we have to go through the intermediate nodes anyway. So, a lot of derivatives would be reused while calculating derivative of prediction with respect to some other node. Therefore, we can store these derivatives of deepest layers like $\\frac{∂p}{∂h1}$ in the above network , skip unnecessary computations and use previous ones by substituting wherever necessary. This is called reverse-mode differentiation or in neural networks as back propagation. It works fast as we compute value of each edge only once when we go along it, store it and reuse it later. \n",
    "\n",
    "There are 2 kinds of passes in back propagation:\n",
    "Forward pass: Made in original computational graph. We need forward pass in order to find the point at which we need to calculate the derivative during backward pass. \n",
    "Backward pass: made in graph of derivatives.\n",
    "To implement a sigmoid activation and backpropagation we need to implement forward pass and backward pass. For forward pass we take all our inputs and apply sigmoid activation function with numpy. But backward pass is a little more complex. It is made on graph of derivatives. We should tell how the derivative changes when we go through the sigmoid node in the opposite direction. It will take input at which we would be taking our derivative and also an incoming gradient, calculate the sigmoid function of input and multiply the derivative of sigmoid function with the incoming gradient. So by chain rule during backward pass the product accumulates automatically along the path.\n",
    "\n",
    "For MLP implementation we treat a dense layer as matrix multiplication. Matrix multiplication with numpy is much faster than python loops. We can do matrix multiplication efficiently on GPU.\n",
    "\n",
    "Forward pass for dense layer would be taking the inputs and their weights and doing a matrix multiplication of them both:\n",
    "$XW = Z$\n",
    "\n",
    "``` python\n",
    "def forward_pass(X,W):\n",
    "    return X.dot(W)\n",
    "```\n",
    "Backward pass takes an additional incoming gradient to calculate dx and dw efficiently. We multiply incoming gradient by W transpose or X transpose to get dX or dW respectively.\n",
    "$\\frac{∂L_b}{∂X} = \\frac{∂L}{∂Z} W^T, \\frac{∂L_b}{∂W} = X^T \\frac{∂L}{∂Z} $\n",
    "``` python\n",
    "def backward_pass(X,W, dZ):\n",
    "    dX = dZ.dot(W.T)\n",
    "    dW = X.T.dot(dZ)\n",
    "    return dX,dW\n",
    "```\n",
    "One reason why we follow the approach of using ∂L/∂Z in backward pass interface is to calculate dX and dW efficiently, otherwise we need to compute ∂Z/∂X or ∂Z/∂W which is complicated as it a matrix derivative with respect to another matrix. Therefore, we have incoming gradient in backward pass to skip unnecessary tensor computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow** is a tool to describe computational graphs. The foundation of computation in TF is a graph object with a network of nodes each representing one operation, connected to each other as inputs and outputs. TF is also a runtime for execution of these graphs. We can execute them in CPU, GPU or in one node or distributed mode environment.  \n",
    "Input as well as output to these networks would be a collection of tensors (multi-dimensional arrays). Therefore, we have a graph of operations that transforms tensors into another tensor so its basically a flow of tensors. Hence called tensor flow.  \n",
    "Input can be of 3 types:  \n",
    "Placeholder: Placeholder for a tensor which will be fed during graph execution  \n",
    "Variable: tensor with some value that is stored and updated during execution like weight matrix in MLP  \n",
    "Constant: tensor with a value that cannot be changed  \n",
    "Tensorflow creates a default graph when imported. All operations go there by default. We can access it by tf.get_default_graph() which returns an instance of tf.graph().  \n",
    "To clear it we use tf.reset_default_graph.  \n",
    "To get the operations we used tf.get_default_graph().get_operations().  \n",
    "To get output of a particular operation tf.get_default_graph().get_operations().outputs  \n",
    "We define a graph in Python but the operations are written in C++ and executed on either CPU or a GPU or a tensor processing unit because python is very slow. However the graph can be executed in the recent versions of tf using eager execution mode. \n",
    "## Runnning a graph\n",
    "A tf.session object encapsulates the environment in which tf.operation objects are executed and tf.tensor objects are evaluated\n",
    "```python\n",
    "#creating a session object\n",
    "s=tf.interactivesession() \n",
    "#define a graph\n",
    "a = tf.constant(5.0) \n",
    "b = tf.constant(6.0)\n",
    "c=a*b \n",
    "#run session \n",
    "s.run(c)\n",
    "```\n",
    "A session object owns necessary resources like variable objects to execute the graph and runs it on CPU, GPU, etc. It is important to release these resources at the end when they are no longer required with *tf.Session.close()*. One thing to note about variable is that we execute the variable in different environment like on GPU for example. So we need to initialize its value on GPU and for that we need to run some code to get initial value to the graph execution environment using *tf.global_variables_initializer()*.  \n",
    "## Training a model with tensorflow\n",
    "There are optimizers in tensors to do back propagation automatically. We don’t have to specify all the optimized variables. Tensorflow knows the graph and which variables are needed to compute the function, so we do not need to specify variables that help in minimization. However, if we do not want a specific variable to be trained we explicitly need to mention in the argument *trainable*.  \n",
    "To get all trainable variables - tf.trainable_variables().  \n",
    "Tensorboard is a visual tool to log, see plots of loss function or any other statistics. To do this we add summaries.  \n",
    "Tensorflow allows to checkpoint your graph but only the variable values are stored and we need to define the graph exactly the same way we did before restoring a checkpoint.  \n",
    "Tf.train.saver(tf.trainable_variables()).save to save variables’ state  \n",
    "Tf.train.saver(tf.trainable_variables()).last_checkpoints to list last checkpoints  \n",
    "Tf.train.saver(tf.trainable_variables()).restore a previous checkpoint  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST digits classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/mnist_sample.png\" style=\"width:5%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're using TF 1.2.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(\"We're using TF\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import matplotlib_utils\n",
    "from importlib import reload\n",
    "reload(matplotlib_utils)\n",
    "\n",
    "import keras_utils\n",
    "from keras_utils import reset_tf_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the data\n",
    "\n",
    "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
    "We will train a classifier on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import preprocessed_mnist\n",
    "# loading the train validation and test datasets\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset_from_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train [shape (50000, 28, 28)] sample patch:\n",
      " [[ 0.          0.29803922  0.96470588  0.98823529  0.43921569]\n",
      " [ 0.          0.33333333  0.98823529  0.90196078  0.09803922]\n",
      " [ 0.          0.33333333  0.98823529  0.8745098   0.        ]\n",
      " [ 0.          0.33333333  0.98823529  0.56862745  0.        ]\n",
      " [ 0.          0.3372549   0.99215686  0.88235294  0.        ]]\n",
      "A closeup of a sample patch:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25e54d6fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the whole sample:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25e543cb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train [shape (50000,)] 10 samples:\n",
      " [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# X contains rgb values divided by 255 (there are 0 to 255 different color intensities and we wish to normalise these values)\n",
    "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
    "print(\"A closeup of a sample patch:\")\n",
    "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"And the whole sample:\")\n",
    "plt.imshow(X_train[1], cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model\n",
    "\n",
    "The task is to train a linear classifier $\\vec{x} \\rightarrow y$ with SGD using TensorFlow.\n",
    "\n",
    "We calculate a logit (a linear transformation) $z_k$ for each class: \n",
    "$$z_k = \\vec{x} \\cdot \\vec{w_k} + b_k \\quad k = 0..9$$\n",
    "\n",
    "And transform logits $z_k$ to valid probabilities $p_k$ with softmax: \n",
    "$$p_k = \\frac{e^{z_k}}{\\sum_{i=0}^{9}{e^{z_i}}} \\quad k = 0..9$$\n",
    "\n",
    "We will use a cross-entropy loss to train our multi-class classifier:\n",
    "$$\\text{cross-entropy}(y, p) = -\\sum_{k=0}^{9}{\\log(p_k)[y = k]}$$ \n",
    "\n",
    "where \n",
    "$$\n",
    "[x]=\\begin{cases}\n",
    "       1, \\quad \\text{if $x$ is true} \\\\\n",
    "       0, \\quad \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Cross-entropy minimization pushes $p_k$ close to 1 when $y = k$, which is what we want.\n",
    "\n",
    "Following are the steps:\n",
    "* Flatten the images (28x28 -> 784) with `X_train.reshape((X_train.shape[0], -1))` to simplify our linear model implementation\n",
    "* Use a matrix placeholder for flattened `X_train`\n",
    "* Convert `y_train` to one-hot encoded vectors that are needed for cross-entropy\n",
    "* Use a shared variable `W` for all weights (a column $\\vec{w_k}$ per class) and `b` for all biases.\n",
    "* Calculate validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train_flat = X_train.reshape((X_train.shape[0], -1)) \n",
    "print(X_train_flat.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
    "print(X_val_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]] [5 0 4]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "#perform one-hot encoding\n",
    "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_train_oh[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this again if you remake your graph\n",
    "s = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters: W and b\n",
    "W = tf.get_variable(name=\"weights\", shape = [784,10]) # shape[0] should match with the number of features (784)\n",
    "b = tf.get_variable(name=\"biases\", shape = [10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for the input data\n",
    "input_X =  tf.placeholder(dtype=\"float32\",shape = [None,784]) # tf.placeholder(...) for flat X with shape[0] = None for any batch size\n",
    "input_y =  tf.placeholder(dtype=\"float32\",shape = [None,10]) # tf.placeholder(...) for one-hot encoded true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "logits = tf.matmul(input_X,W) + b # logits for input_X, resulting shape should be [input_X.shape[0], 10]\n",
    "probas = tf.nn.softmax(logits) # apply softmax to logits\n",
    "classes = tf.argmax(probas,axis=1) # apply tf.argmax to find a class index with highest probability\n",
    "# Loss should be a scalar number: average loss over all the objects with tf.reduce_mean().\n",
    "# Use tf.nn.softmax_cross_entropy_with_logits on top of one-hot encoded input_y and logits.\n",
    "# It is identical to calculating cross-entropy on top of probas, but is more numerically friendly.\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=input_y))# cross-entropy loss\n",
    "\n",
    "# Use a default tf.train.AdamOptimizer to get an SGD step\n",
    "step = tf.train.AdamOptimizer(0.001).minimize(loss) # optimizer step that minimizes the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10) (?, 10) (?,)\n",
      "0.83564 0.8572\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.87176 0.8869\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.88606 0.8973\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.8944 0.9058\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.90002 0.9101\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.90454 0.9129\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.90772 0.9155\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.9104 0.9173\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.91244 0.918\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.9144 0.9198\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.91572 0.9214\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.91694 0.9218\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.91788 0.9225\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.91884 0.9232\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.91936 0.9235\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92042 0.9239\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.9211 0.9239\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92184 0.9243\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92244 0.9255\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92286 0.9259\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92336 0.9261\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92392 0.9263\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92432 0.927\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92486 0.927\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92544 0.9273\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92598 0.9277\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.9264 0.9276\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92678 0.9279\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92708 0.9282\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92742 0.9282\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92782 0.9283\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.928 0.9291\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92814 0.9297\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92844 0.9298\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92858 0.9301\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92878 0.9299\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92916 0.93\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.92964 0.93\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.9299 0.93\n",
      "(?, 10) (?, 10) (?,)\n",
      "0.93018 0.9304\n"
     ]
    }
   ],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
    "    \n",
    "    batch_losses = []\n",
    "    for batch_start in range(0, X_train_flat.shape[0], BATCH_SIZE):  #512 images/records per loop until 50000 images\n",
    "        _, batch_loss = s.run([step, loss], {input_X: X_train_flat[batch_start:batch_start+BATCH_SIZE], \n",
    "                                             input_y: y_train_oh[batch_start:batch_start+BATCH_SIZE]})     \n",
    "        batch_losses.append(batch_loss) # cross entropy loss is accumulated for every batch size \n",
    "    train_loss = tf.reduce_mean(batch_losses) \n",
    "    val_loss = s.run(loss, {input_X: X_val_flat, input_y: y_val_oh})  # loss minimises every epoch as the w's change to optimize loss\n",
    "    train_accuracy = accuracy_score(y_train, s.run(classes, {input_X: X_train_flat})) #mismatches between y_train \n",
    "                                                                                      #and class with highest probability from predictions\n",
    "    print(logits.shape,probas.shape,classes.shape)\n",
    "    valid_accuracy = accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})) #similarly for validation classes and their prediction\n",
    "    print(train_accuracy, valid_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we've coded a dense layer with matrix multiplication by hand. \n",
    "But this is not convenient, you have to create a lot of variables and your code becomes a mess. \n",
    "In TensorFlow there's an easier way to make a dense layer:\n",
    "```python\n",
    "hidden1 = tf.layers.dense(inputs, 256, activation=tf.nn.sigmoid)\n",
    "```\n",
    "That will create all the necessary variables automatically.  \n",
    "Here we can also choose an activation function (which we need it for a hidden layer).  \n",
    "Here we define the MLP with 2 hidden layers and restart training with the cell above.  \n",
    "You're aiming for higher validation accuracy (~0.97)  here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(input_X, 256, activation=tf.nn.sigmoid)\n",
    "hidden2 = tf.layers.dense(hidden1, 256, activation=tf.nn.sigmoid)\n",
    "logits = tf.layers.dense(hidden2,input_y.shape[1])\n",
    "\n",
    "probas = tf.nn.softmax(logits) \n",
    "classes = tf.argmax(probas,axis=1) # apply tf.argmax to find a class index with highest probability\n",
    "# Loss should be a scalar number: average loss over all the objects with tf.reduce_mean().\n",
    "# Use tf.nn.softmax_cross_entropy_with_logits on top of one-hot encoded input_y and logits.\n",
    "# It is identical to calculating cross-entropy on top of probas, but is more numerically friendly (read the docs).\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=input_y))# cross-entropy loss\n",
    "\n",
    "# Use a default tf.train.AdamOptimizer to get an SGD step\n",
    "step = tf.train.AdamOptimizer(0.001).minimize(loss) #optimizer step that minimizes the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871 0.8854\n",
      "0.90756 0.9146\n",
      "0.92026 0.927\n",
      "0.93046 0.9333\n",
      "0.93764 0.9408\n",
      "0.94374 0.946\n",
      "0.94908 0.9496\n",
      "0.95366 0.9542\n",
      "0.95782 0.9569\n",
      "0.9613 0.9605\n",
      "0.96506 0.9617\n",
      "0.96828 0.9637\n",
      "0.97112 0.9651\n",
      "0.9739 0.9666\n",
      "0.97654 0.9681\n",
      "0.97826 0.9692\n",
      "0.97994 0.9699\n",
      "0.98168 0.9714\n",
      "0.9833 0.972\n",
      "0.98474 0.9721\n",
      "0.98594 0.9725\n",
      "0.98706 0.9735\n",
      "0.98812 0.9739\n",
      "0.98924 0.9745\n",
      "0.99018 0.9747\n",
      "0.99108 0.9759\n",
      "0.992 0.9761\n",
      "0.99274 0.9768\n",
      "0.99322 0.9772\n",
      "0.99384 0.9771\n",
      "0.99426 0.977\n",
      "0.99466 0.9771\n",
      "0.99498 0.9775\n",
      "0.99552 0.9776\n",
      "0.99614 0.9779\n",
      "0.99674 0.9779\n",
      "0.9974 0.9781\n",
      "0.99782 0.9786\n",
      "0.99826 0.9783\n",
      "0.99836 0.9779\n"
     ]
    }
   ],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
    "    \n",
    "    batch_losses = []\n",
    "    for batch_start in range(0, X_train_flat.shape[0], BATCH_SIZE): #512 images/records per loop until 50000 images\n",
    "        _, batch_loss = s.run([step, loss], {input_X: X_train_flat[batch_start:batch_start+BATCH_SIZE], \n",
    "                                             input_y: y_train_oh[batch_start:batch_start+BATCH_SIZE]})\n",
    "        batch_losses.append(batch_loss) # cross entropy loss is accumulated for every batch size \n",
    "    train_loss = tf.reduce_mean(batch_losses)\n",
    "    val_loss = s.run(loss, {input_X: X_val_flat, input_y: y_val_oh})  # loss minimises every epoch as the w's change to optimize loss\n",
    "    train_accuracy = accuracy_score(y_train, s.run(classes, {input_X: X_train_flat}))  #mismatches between y_train \n",
    "                                                                                       #and class with highest probability from predictions\n",
    "    valid_accuracy = accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})) #similarly for validation classes and their prediction\n",
    "    print(train_accuracy, valid_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
